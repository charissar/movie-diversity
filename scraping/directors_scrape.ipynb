{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports = []\n",
    "import_numbers = [1,2,3,5,6,7,10,11,12,15,16,17,20,21,22,25,26,27,30,31,32,35,36,37,40,41,42,45,46,47]   \n",
    "for num in import_numbers:\n",
    "    imports.append('movie_details_exports/movie_details_export_%s.json' %(str(num)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "details_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imp in imports:\n",
    "    details_df = details_df.append(pd.read_json(imp, orient='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(details_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List of Unique Directors and URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directors_df = details_df[['Director_URL','Director']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directors_df = directors_df.dropna(subset = ['Director_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(directors_df) --> 29711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directors_df = directors_df.drop_duplicates(subset='Director_URL',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(directors_df) --> 11939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "director_urls = list(directors_df['Director_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "director_urls = sorted(director_urls, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Director Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Director Details\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "directors_scraped = 0\n",
    "director_details = {}\n",
    "director_index = 0                     \n",
    "prefix = 'http://www.imdb.com'\n",
    "export_number = 1\n",
    "export_no = str(export_number)\n",
    "load_attempts = 0\n",
    "\n",
    "def director_scraper():\n",
    "    global director_index\n",
    "    \n",
    "    print(str(datetime.now()),': working on export ' + export_no)\n",
    "    \n",
    "    for url in director_urls[director_index:2000]:\n",
    "#         print(url)\n",
    "        if (director_index+1) % 20 == 0:\n",
    "            time.sleep(1.5)\n",
    "            print('%d: %s' %(director_index+1, url))\n",
    "        page = request_page(url)\n",
    "        get_director_details(url, page)\n",
    "        director_index += 1\n",
    "    \n",
    "    export_to_json(director_details)\n",
    "    print(\"Done\")\n",
    "    \n",
    "def request_page(url):\n",
    "    global load_attempts\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return(response.text)\n",
    "        load_attempts = 0\n",
    "    else:\n",
    "        print(response.status_code,'for %s' %(url))\n",
    "        if response.status_code != 404:\n",
    "            if load_attempts == 10:\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                time.sleep(10)\n",
    "            request_page(url)\n",
    "            load_attempts += 1\n",
    "\n",
    "def get_director_details(url, page):\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    director_details[url] = {}\n",
    "    \n",
    "    gender = soup.find(id='name-job-categories')\n",
    "    gender = gender.findAll('a') if gender else None\n",
    "    gender = [x['href'] for x in gender] if gender else None\n",
    "    gender = [re.sub('[^a-z]+','',x) for x in gender] if gender else None\n",
    "    if gender:\n",
    "        if 'actor' in gender:\n",
    "            gender = 'Male'\n",
    "        elif 'actress' in gender:\n",
    "            gender = 'Female'\n",
    "        else:\n",
    "            gender = soup.find(id='name-bio-text')\n",
    "            gender = gender.text if gender else None\n",
    "            gender = re.sub('\\n', '', gender) if gender else None\n",
    "            gender = re.sub('  +',' ', gender) if gender else None\n",
    "            gender = gender.split(' ') if gender else None\n",
    "            male_count = gender.count('His') + gender.count('his') + gender.count('He') + gender.count('he') if gender else None\n",
    "            female_count = gender.count('Her') + gender.count('her') + gender.count('She') + gender.count('she') if gender else None\n",
    "            if gender:\n",
    "                if male_count > female_count:\n",
    "                    gender = 'Male'\n",
    "                elif female_count > male_count:\n",
    "                    gender = 'Female'\n",
    "                else:\n",
    "                    gender = None\n",
    "    director_details[url]['Gender'] = gender\n",
    "    \n",
    "    \n",
    "    director_name = soup.find('h1')\n",
    "    director_name = director_name.text if director_name else None\n",
    "    director_name = director_name.strip() if director_name else None\n",
    "    director_name = director_name.split('\\n') if director_name else None\n",
    "\n",
    "    deathyear = director_name[-1] if len(director_name)>=2 else None\n",
    "    deathyear = deathyear.split('â€“')[-1] if deathyear else None\n",
    "    deathyear = re.sub('[^0-9]+','',deathyear) if deathyear else None\n",
    "    deathyear = int(deathyear) if deathyear else None\n",
    "    director_details[url]['Deathyear'] = deathyear\n",
    "    \n",
    "\n",
    "    director_name = director_name[0] if director_name else None\n",
    "    director_details[url]['Name'] = director_name    \n",
    "    \n",
    "    \n",
    "    born_info = soup.find(id='name-born-info')\n",
    "\n",
    "    birthdate = born_info.find('time') if born_info else None\n",
    "    if birthdate:\n",
    "        if birthdate.has_attr('datetime'):\n",
    "            birthdate = birthdate['datetime']\n",
    "        else:\n",
    "            birthdate = None\n",
    "    director_details[url]['Birthdate'] = birthdate\n",
    "\n",
    "    birthyear = birthdate[0:4] if birthdate and len(birthdate)>=8 else None\n",
    "    birthyear = int(birthyear) if birthyear else None\n",
    "    director_details[url]['Birthyear'] = birthyear\n",
    "\n",
    "    age_at_death = deathyear-birthyear if deathyear and birthyear else None\n",
    "    director_details[url]['Age at Death'] = age_at_death\n",
    "\n",
    "    \n",
    "    birthplace = born_info.findAll('a') if born_info else None\n",
    "    birthplace = birthplace[-1] if birthplace else None\n",
    "    birthplace = birthplace.text if birthplace else None\n",
    "    director_details[url]['Birthplace'] = birthplace\n",
    "\n",
    "    \n",
    "    birthcountry = birthplace.split(', ') if birthplace else None\n",
    "    birthcountry = birthcountry[-1] if birthcountry and len(birthcountry)>1 else None\n",
    "    director_details[url]['Birth Country'] = birthcountry\n",
    "\n",
    "    award_highlight = soup.find('div', class_='article highlighted')\n",
    "    award_highlight = award_highlight.findAll('span') if award_highlight else None\n",
    "    award_highlight = [x.text for x in award_highlight] if award_highlight else None\n",
    "    award_highlight = [x.strip() for x in award_highlight] if award_highlight else None\n",
    "    award_highlight = [x.split('\\n') for x in award_highlight] if award_highlight else None\n",
    "    award_highlight = [''.join(x) for x in award_highlight[:-1]] if award_highlight else None\n",
    "    award_highlight = [re.sub(\"  +\", \" \", x) for x in award_highlight] if award_highlight else None\n",
    "    award_highlight = ' '.join(award_highlight) if award_highlight else None    \n",
    "    director_details[url]['Award Info'] = award_highlight\n",
    "    \n",
    "    writing_roles = soup.find(id='filmo-head-writer')\n",
    "    writing_roles = writing_roles.text if writing_roles else None\n",
    "    writing_roles = writing_roles.strip() if writing_roles else None\n",
    "    writing_roles = writing_roles.split('(')[-1] if writing_roles else None\n",
    "    writing_roles = re.sub('[^0-9]+','',writing_roles) if writing_roles else None\n",
    "    writing_roles = int(writing_roles) if writing_roles else None\n",
    "    director_details[url]['Writing Roles'] = writing_roles\n",
    "    \n",
    "    editing_roles = soup.find(id='filmo-head-editor')\n",
    "    editing_roles = editing_roles.text if editing_roles else None\n",
    "    editing_roles = editing_roles.strip() if editing_roles else None\n",
    "    editing_roles = editing_roles.split('(')[-1] if editing_roles else None\n",
    "    editing_roles = re.sub('[^0-9]+','',editing_roles) if editing_roles else None\n",
    "    editing_roles = int(editing_roles) if editing_roles else None\n",
    "    director_details[url]['Editing Roles'] = editing_roles\n",
    "    \n",
    "    acting_roles = soup.find(id='filmo-head-actor')\n",
    "    acting_roles = acting_roles.text if acting_roles else None\n",
    "    acting_roles = acting_roles.strip() if acting_roles else None\n",
    "    acting_roles = acting_roles.split('(')[-1] if acting_roles else None\n",
    "    acting_roles = re.sub('[^0-9]+','',acting_roles) if acting_roles else None\n",
    "    acting_roles = int(acting_roles) if acting_roles else None\n",
    "    director_details[url]['Acting Roles'] = acting_roles\n",
    "    \n",
    "    directing_roles = soup.find(id='filmo-head-director')\n",
    "    directing_roles = directing_roles.text if directing_roles else None\n",
    "    directing_roles = directing_roles.strip() if directing_roles else None\n",
    "    directing_roles = directing_roles.split('(')[-1] if directing_roles else None\n",
    "    directing_roles = re.sub('[^0-9]+','',directing_roles) if directing_roles else None\n",
    "    directing_roles = int(directing_roles) if directing_roles else None\n",
    "    director_details[url]['Directing Roles'] = directing_roles  \n",
    "    \n",
    "    producing_roles = soup.find(id='filmo-head-producer')\n",
    "    producing_roles = producing_roles.text if producing_roles else None\n",
    "    producing_roles = producing_roles.strip() if producing_roles else None\n",
    "    producing_roles = producing_roles.split('(')[-1] if producing_roles else None\n",
    "    producing_roles = re.sub('[^0-9]+','',producing_roles) if producing_roles else None\n",
    "    producing_roles = int(producing_roles) if producing_roles else None\n",
    "    director_details[url]['Producing Roles'] = producing_roles\n",
    "    \n",
    "    cinematographic_roles = soup.find(id='filmo-head-cinematographer')\n",
    "    cinematographic_roles = cinematographic_roles.text if cinematographic_roles else None\n",
    "    cinematographic_roles = cinematographic_roles.strip() if cinematographic_roles else None\n",
    "    cinematographic_roles = cinematographic_roles.split('(')[-1] if cinematographic_roles else None\n",
    "    cinematographic_roles = re.sub('[^0-9]+','',cinematographic_roles) if cinematographic_roles else None\n",
    "    cinematographic_roles = int(cinematographic_roles) if cinematographic_roles else None\n",
    "    director_details[url]['Cinematographic Roles'] = cinematographic_roles\n",
    "    \n",
    "    tv_appearances = soup.find(id='filmo-head-self')\n",
    "    tv_appearances = tv_appearances.text if tv_appearances else None\n",
    "    tv_appearances = tv_appearances.strip() if tv_appearances else None\n",
    "    tv_appearances = tv_appearances.split('(')[-1] if tv_appearances else None\n",
    "    tv_appearances = re.sub('[^0-9]+','',tv_appearances) if tv_appearances else None\n",
    "    tv_appearances = int(tv_appearances) if tv_appearances else None\n",
    "    director_details[url]['TV Appearances'] = tv_appearances\n",
    "    \n",
    "\n",
    "    height = soup.find('h2',text=re.compile('Personal Details'))\n",
    "    height = height.parent if height else None\n",
    "    height = height.find(id='details-height') if height else None\n",
    "    height = height.text if height else None\n",
    "    height = height.split('(')[-1] if height else None\n",
    "    height = re.sub('[^0-9.]+','',height) if height else None\n",
    "    director_details[url]['Height in m'] = height\n",
    "\n",
    "    publicity_listings = soup.find('h2',text=re.compile('Personal Details'))\n",
    "    publicity_listings = publicity_listings.parent if publicity_listings else None\n",
    "    publicity_listings = publicity_listings.find(id='details-publicity-listings') if publicity_listings else None\n",
    "    publicity_listings = publicity_listings.text if publicity_listings else None\n",
    "    publicity_listings = re.sub('[^0-9]+',' ',publicity_listings) if publicity_listings else None\n",
    "    publicity_listings = publicity_listings.strip() if publicity_listings else None\n",
    "    publicity_listings = publicity_listings.split(' ') if publicity_listings else None\n",
    "    publicity_listings = [int(x) for x in publicity_listings] if publicity_listings else None\n",
    "    publicity_listings = sum(publicity_listings) if publicity_listings else None\n",
    "    director_details[url]['Publicity Listings'] = publicity_listings\n",
    "        \n",
    "    star_sign = soup.find('h4',text=re.compile('Star Sign'))\n",
    "    star_sign = star_sign.parent if star_sign else None\n",
    "    star_sign = star_sign.text if star_sign else None\n",
    "    star_sign = star_sign.strip() if star_sign else None\n",
    "    star_sign = star_sign.split('\\n')[-1] if star_sign else None\n",
    "    director_details[url]['Star Sign'] = star_sign\n",
    "\n",
    "    go_to_next_and_export()\n",
    "\n",
    "def go_to_next_and_export():\n",
    "    global directors_scraped\n",
    "    global director_details\n",
    "    \n",
    "    directors_scraped+=1\n",
    "\n",
    "    if len(director_details) == 1000:                                         \n",
    "        export_to_json(director_details)\n",
    "        directors_scraped = 0\n",
    "        director_details = {}\n",
    "\n",
    "def export_to_json(director_details):\n",
    "    global export_no\n",
    "    global export_number\n",
    "    with open('director_details_exports/director_details_export_%s.json' %(export_no),'w')  as f:\n",
    "        json.dump(director_details,f)\n",
    "\n",
    "    export_number += 1\n",
    "    export_no = str(export_number)\n",
    "    print(str(datetime.now()),': working on export ' + export_no)\n",
    "\n",
    "start = time.time()\n",
    "director_scraper()\n",
    "end = time.time()\n",
    "print(round(end-start,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
